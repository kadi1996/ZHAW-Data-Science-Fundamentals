{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression & Gradient Descent\n",
    "\n",
    "In this notebook we will work with the diabetes dataset \n",
    "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
    "https://web.stanford.edu/~hastie/Papers/LARS/diabetes.data (raw data, not standardised)\n",
    "\n",
    "The data set contains 10 baseline variables (X1, X2, X3....X10) for 442 patients with diabetes: age, sex, body mass index, average blood pressure, and six blood serum measurements. The target variable (y) indicates the progression of the dieases one year after baseline.\n",
    "\n",
    "Luckily, this database is available in sklearn, so we need only to load it. \n",
    "Even more luckily, the sklearn database is already standardised (compare with the raw data above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np # for numerical operations\n",
    "from sklearn import datasets, linear_model # for linear regression\n",
    "from sklearn.model_selection import train_test_split # for splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "# load_diabetes() is a default dataset in sklearn\n",
    "# function returns the data and the target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a subset of the dataset, specifically only the variables: body mass index, average blood pressure and one blood serum measurement. Feel free to include more dimensions in your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_X = diabetes_X[:,2:5] \n",
    "#select only the 3 relevant features we want to consider\n",
    "# it selects the third through fifth features from the original set of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing, we split our dataset in training set and test set (z.B 80 data points are left as a test).\n",
    "We will fit(train) our model on the training dataset and then we will evaluate how they really perform on test data.\n",
    "The performance on the test data is fundamental to see how models generalise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training and test sets -> preparing for training in a ML model (like linear regression model) on the diabetes_X_train and diabetes_y_train datasets and then evaluate its performance using diabetes_X_test and diabetes_y_test\n",
    "# splitting feature array diabetes_X and target array diabetes_Y in two parts\n",
    "\n",
    "diabetes_X_train = diabetes_X[:-80] # [:-80] means \"take everything except the last 80 entries\"\n",
    "diabetes_X_test = diabetes_X[-80:] # [-80:]means \"take the last 80 entries\"\n",
    "\n",
    "\n",
    "diabetes_y_train = diabetes_y[:-80]\n",
    "diabetes_y_test = diabetes_y[-80:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we want to first compare several methods to determine the best parameters for our model. \n",
    "Let's start with the LinearRegression() function from sklearn.\n",
    "The LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for \"least squares\"), this compute the optimal paramters'values using the least square exact method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters (theta_1, theta_2, theta_3, theta_0): \n",
      " [805.69232777 363.70203872  52.14450364] 152.53452637832743\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression() # initializes a linear regression model from linear_model. LinearRegression() is a constructor that creates an object representing the regression model \n",
    "\n",
    "\n",
    "regr.fit(diabetes_X_train, diabetes_y_train) # fits the model to the training data using fit() method. fit() adjusts weights of model (coefficients and intercept) to minimize difference between predicted and actual outcomes \n",
    "\n",
    "# The model's parameters\n",
    "print('Parameters (theta_1, theta_2, theta_3, theta_0): \\n', regr.coef_, regr.intercept_) # prints coefficients and intercept of the model\n",
    "# coefficient represent the relationship between the features and the target variable\n",
    "# intercept represents the predicted output when all features are zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to compute the same parameters using the Batch Gradient Descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent = optimization algorithm used to minimize some function by iteratively moving towards the minimum of the function. In Linear Regression we want to minimize the cost function (= measure of how wrong the model predictions are aka sum of squarred errors)\n",
    "\n",
    "# With the Batch Gradient Descent we calculate the gradient of the cost function to take a step towards reducing the cost function \n",
    "\n",
    "eta = 0.1  # sets the learning rate = hyperparameter that controls how much the weights in the model should change in response to the estimated error each time the model weights are updated \n",
    "n_iterations = 100000 #try with 1000, 10000, 100000... number of iterations\n",
    "N = len(diabetes_X) # calculates nr of observations in dataset diabetes_X and stores it in N\n",
    "\n",
    "theta_0 = np.random.randn(1) # random initialization of the intercept\n",
    "theta_1 = np.random.randn(1) # initializes coefficient\n",
    "theta_2 = np.random.randn(1)\n",
    "theta_3 = np.random.randn(1)\n",
    "\n",
    "X1= diabetes_X_train[:,0] # selects the first feature from the training set and stores it in X1\n",
    "X2= diabetes_X_train[:,1] # selects the second feature from the training set and stores it in X2\n",
    "X3= diabetes_X_train[:,2] # selects the third feature from the training set and stores it in X3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code here the formulas for the batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations): # sets a loop that executes the gradient calculation and parameter update n_iterations times -> each iteration represents one complete pass through the dataset to update the parameters \n",
    "\n",
    "    \n",
    "    gradient_theta_3 = 1/N * np.sum((theta_0 + theta_1*X1 + theta_2*X2 + theta_3*X3  - diabetes_y_train)*X3) # calculates the gradient of the cost function with respect to the parameter theta_3. It computes how much the prediction error changes with a small change in theta_3 \n",
    "\n",
    "    gradient_theta_2 = 1/N * np.sum((theta_0 + theta_1*X1 + theta_2*X2 + theta_3*X3  - diabetes_y_train)*X2) # calculates the gradient of the cost function with respect to the parameter theta_2\n",
    "\n",
    "    gradient_theta_1 = 1/N * np.sum((theta_0 + theta_1*X1 + theta_2*X2 + theta_3*X3  - diabetes_y_train)*X1) # calculates the gradient of the cost function with respect to the parameter theta_1\n",
    "\n",
    "    gradient_theta_0 = 1/N * np.sum((theta_0 + theta_1*X1 + theta_2*X2 + theta_3*X3  - diabetes_y_train)) # calculates the gradient of the cost function with respect to the parameter theta_0\n",
    "    \n",
    "    \n",
    "    # each parameter is updated by subtracting the product of the learning rate (eta) and the corresponding gradient \n",
    "    theta_3 = theta_3 - eta * gradient_theta_3\n",
    "    theta_2 = theta_2 - eta * gradient_theta_2                                \n",
    "    theta_1 = theta_1 - eta * gradient_theta_1\n",
    "    theta_0 = theta_0 - eta * gradient_theta_0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[805.68580321] [363.70740419] [52.1456567] [152.53451878]\n"
     ]
    }
   ],
   "source": [
    "#Let's see hwo the parameters look like \n",
    "print(theta_1,theta_2,theta_3, theta_0)\n",
    "\n",
    "# results are very close to each other \n",
    "# indicates that the Batch Gradient Descent, with chosen settings for learning rate and number of iterations, has converged well towards the optimal values found by the OLS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the values obtained with the function LinearRegression().\n",
    "Try to change the number of iterations and/or eta to get closer to the exact values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "We try now to see how the results change if we use the SGD approach. We use here the sklearn function SGDRegressor.\n",
    "Check out here the syntax\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
    "General Infos:\n",
    "https://scikit-learn.org/stable/modules/sgd.html#regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor # importing the Stochastic Gradient Descent Regressor from sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor(max_iter=1000000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor(max_iter=1000000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDRegressor(max_iter=1000000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg = SGDRegressor(max_iter=1000000, eta0=0.01) # initializes the SGDRegressor with a maximum number of iterations and learning rate\n",
    "sgd_reg.fit(diabetes_X_train, diabetes_y_train) #fits model to the training data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([633.56474915, 370.14817085, 121.92385238]), array([152.37102015]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.coef_, sgd_reg.intercept_\n",
    "\n",
    "# results are a bit different \n",
    "# SGD might require more iterations to converge to the optimal values found by the OLS or BGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare with the previous solutions? Try to adapt also here eta and number of iterations. \n",
    "How does it compare in terms of velocity with the Batch Gradient Descent?\n",
    "\n",
    "Try to run the previous 2 cells again. Do you get always the same result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you found the best parameters for your linear model, you can use them on the test data to predict values and evaluate how well your model perform, that is how much your predictions are off from the true y values of your test data.\n",
    "Let's start with the model obtained with LinearRegression() and then do the same with the linear regression obtained via SGD. \n",
    "How do the 2 compare on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_y_pred_EasyLinReg = regr.predict(diabetes_X_test) # predicts the target values for the test set using the model trained with the LinearRegression() function\n",
    "diabetes_y_pred_SGD = sgd_reg.predict(diabetes_X_test) # predicts the target values using the model trained with SGDRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression() Mean squared error: 3327.59\n",
      "SGD() Mean squared error: 3353.03\n"
     ]
    }
   ],
   "source": [
    "print('LinearRegression() Mean squared error: %.2f' % mean_squared_error(diabetes_y_test, diabetes_y_pred_EasyLinReg)) # calculates and prints the MSE for LinearRegression() model\n",
    "print('SGD() Mean squared error: %.2f' % mean_squared_error(diabetes_y_test, diabetes_y_pred_SGD)) # calculates and prints the MSE for SGD() model\n",
    "\n",
    "# results are close, indicating that both models are performing similarly on the test set -> LinearRegression() might be a more accurate \n",
    "# Calculating the MSE via LinearRegression() and SGD() on the test data helps evaluation of the model's performance on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: Notebook adapted from A.Geron, Hands On ML with Scikit-Learn, Keras und Tensorflow, O'Reilly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
